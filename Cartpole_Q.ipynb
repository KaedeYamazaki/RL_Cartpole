{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12521,"status":"ok","timestamp":1696668226912,"user":{"displayName":"r rrr","userId":"14222254183072818856"},"user_tz":-540},"id":"iPdUtXDM-UZf","outputId":"9d06aead-f3e6-4ef4-9fa7-9ce6413fb000"},"outputs":[],"source":["# !pip install mujoco\n","# !pip install pip install gymnasium[mujoco]"]},{"cell_type":"markdown","metadata":{},"source":["### 概要\n","Mujoco環境下でのカートポールのQ学習による学習とその推論が行えるサンプル\n","\n","### 注意事項\n","pkl形式で保存されるQTableは，パラメータによっては1GB以上の大きさになるので注意！"]},{"cell_type":"markdown","metadata":{},"source":["# Q学習"]},{"cell_type":"markdown","metadata":{},"source":["#### 構成\n","1. ライブラリのインポート\n","1. ハイパーパラメータの定義\n","1. 環境の用意\n","1. 離散化処理\n","1. Q値の初期化\n","2. メインループ\n","   1. ε-greedy\n","   2. 状態と報酬の取得\n","   3. Q値の更新\n","   4. 状態の更新"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["()\n"]},{"name":"stderr","output_type":"stream","text":["                                       \r"]},{"ename":"AssertionError","evalue":"(0,) (<class 'tuple'>) invalid","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 101\u001b[0m\n\u001b[0;32m     96\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_number_of_steps):  \u001b[38;5;66;03m# 1試行のループ , leave=False\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# print(action,state)\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m     observation, reward, done, _, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;66;03m# 状態を保存\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;66;03m# state_data.append((observation, action, reward)) # 現在\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;66;03m# render_data.append(env.render()) # 次\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;66;03m# episode_data.append(episode)\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     step_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n","File \u001b[1;32mc:\\Users\\KaedeYamazaki\\Desktop\\py_venv\\mujoco_env\\lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n","File \u001b[1;32mc:\\Users\\KaedeYamazaki\\Desktop\\py_venv\\mujoco_env\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\KaedeYamazaki\\Desktop\\py_venv\\mujoco_env\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n","File \u001b[1;32mc:\\Users\\KaedeYamazaki\\Desktop\\py_venv\\mujoco_env\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:208\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[1;34m(env, action)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    210\u001b[0m     result, \u001b[38;5;28mtuple\u001b[39m\n\u001b[0;32m    211\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects step result to be a tuple, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n","File \u001b[1;32mc:\\Users\\KaedeYamazaki\\Desktop\\py_venv\\mujoco_env\\lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:133\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mcontains(\n\u001b[0;32m    134\u001b[0m         action\n\u001b[0;32m    135\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(action)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) invalid\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCall reset before using step method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m     x, x_dot, theta, theta_dot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n","\u001b[1;31mAssertionError\u001b[0m: (0,) (<class 'tuple'>) invalid"]}],"source":["import gymnasium as gym\n","import numpy as np\n","import mujoco\n","import time\n","import pickle\n","import matplotlib.pyplot as plt\n","from tqdm import trange\n","from collections import defaultdict\n","\n","# グラフ描画用の配列\n","record_episode = []\n","record_reward = []\n","record_step = []\n","model = defaultdict(lambda: dict())\n","\n","'''\n","ハイパーパラメータ\n","'''\n","ALPHA = 0.05\n","GAMMA = 0.95 # 割引率\n","EPSILON = 0.1 # ε-greedy法のε\n","max_number_of_steps = 1000  # 1試行のstep数\n","num_consecutive_iterations = 5  # 学習完了評価に使用する平均試行回数\n","num_episodes = 80000  # 総試行回数\n","total_reward_vec = np.zeros(num_consecutive_iterations)  # 各試行の報酬を格納\n","num_planning_steps = 50 #プランニングステップ数\n","\n","start_time = time.time()\n","\n","rng = np.random.RandomState()\n","\n","'''\n","環境の用意\n","'''\n","env = gym.make('InvertedPendulum-v4')#, render_mode=\"human\" CartPole-v1\n","print(env.action_space.shape)\n","observation, info = env.reset()\n","\n","'''\n","離散化\n","'''\n","num_dizitized = 5  # 状態の分割数\n","def bins(clip_min, clip_max, num):\n","    return np.linspace(clip_min, clip_max, num + 1)[1:-1]\n","\n","# 各値を離散値に変換\n","def digitize_state(observation):\n","    cart_pos, cart_v, pole_angle, pole_v = observation\n","    digitized = [\n","        np.digitize(cart_pos, bins=bins(-2.4, 2.4, num_dizitized)),\n","        np.digitize(cart_v, bins=bins(-3.0, 3.0, num_dizitized)),\n","        np.digitize(pole_angle, bins=bins(-0.5, 0.5, num_dizitized)),\n","        np.digitize(pole_v, bins=bins(-2.0, 2.0, num_dizitized))\n","    ]\n","    return sum([x * (num_dizitized**i) for i, x in enumerate(digitized)])\n","\n","\n","q_table = np.random.uniform(low=-1, high=1, size=(num_dizitized**4, 2))\n","\n","\n","'''\n","学習の実行\n","'''\n","# ---epsilon-グリーディ\n","def get_action_q(next_state, episode, epsilon):\n","    if epsilon <= np.random.uniform(0, 1):\n","        next_action = np.argmax(q_table[next_state])\n","    else:\n","        next_action = np.random.choice([0, 1])\n","    return next_action\n","\n","# ---Q学習関連\n","def update_Qtable(q_table, state, action, reward, next_state):\n","    next_Max_Q=max(q_table[next_state][0],q_table[next_state][1] )\n","    q_table[state, action] = (1 - ALPHA) * q_table[state, action] + ALPHA * (reward + GAMMA * next_Max_Q)\n","    return q_table\n","\n","# ---Dyna\n","def add_experience_to_model(state, action, next_state, reward):\n","    model[state][action] = tuple([next_state, reward])\n","\n","def sample_from_model():\n","    state = rng.choice(list(model.keys()))\n","    action = rng.choice(list(model[state].keys()))\n","    next_state, reward = model[state][action]\n","    return state, action, next_state, reward\n","\n","\n","ts = time.time()\n","for episode in trange(num_episodes, leave=False):  # 試行数分繰り返す\n","    # 環境の初期化\n","    state,info = env.reset()\n","    observation = state[0],state[1],state[2],state[3],\n","    state = digitize_state(observation)\n","    action = np.argmax(q_table[state])\n","    episode_reward = 0\n","\n","\n","    for t in trange(max_number_of_steps, leave=False):  # 1試行のループ\n","        # print(action,state)\n","        observation, reward, done, _, info = env.step((action,))\n","        # 状態を保存\n","        # state_data.append((observation, action, reward)) # 現在\n","        # render_data.append(env.render()) # 次\n","        # episode_data.append(episode)\n","        step_num = 0\n","\n","        # 報酬設計\n","        if done:\n","            if t < 195: # step = 195　で終了にしている．\n","                reward = 0  # 倒れたら罰則\n","            else:\n","                reward = 1  # 立ったまま終了時は罰則はなし\n","        else:\n","            reward = 1  # 各ステップで立ってたら報酬追加\n","\n","        episode_reward += reward  # 報酬を追加\n","\n","        # Q-tableを更新する\n","        next_state = digitize_state(observation)  # 観測状態を離散値に変換\n","        q_table = update_Qtable(q_table, state, action, reward, next_state)\n","        #Dyna-Q\n","        add_experience_to_model(state, action, next_state, reward)\n","        for i in range(num_planning_steps):\n","            state_in_model, action_in_model, next_state_in_model, reward_in_model = sample_from_model()\n","            update_Qtable(q_table, state_in_model, action_in_model, next_state_in_model, reward_in_model)\n","            # state, action, next_state, reward\n","\n","        #  行動を選択\n","        action = get_action_q(next_state, episode, EPSILON)\n","\n","        print(action)\n","\n","        state = next_state\n","        step_num += 1\n","        # 終了時の処理\n","        if done:\n","            # if (episode+1)%10==0:\n","            #   print('Episode {0}: {1} steps, reward {2}, mean reward {3:.3f}'.format(episode+1, t+1, episode_reward, total_reward_vec.mean()))\n","            total_reward_vec = np.hstack((total_reward_vec[1:], episode_reward))  # 報酬を記録\n","            record_reward.append(total_reward_vec.mean()) # グラフ描画用に記録\n","            record_episode.append(episode+1)\n","            step_num = t\n","            record_step.append(step_num+1)\n","            break\n","env.close()\n","\n","# Qテーブルを保存する\n","with open('q_table.pkl', 'wb') as f:\n","    pickle.dump(q_table, f)\n","\n","# プロット\n","fig = plt.figure()\n","plt.subplot(1, 2, 1)\n","plt.plot(record_episode, record_reward, color=\"red\")\n","plt.grid()\n","plt.xlabel(\"episode\")\n","plt.ylabel(\"mean reward\")\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(record_episode, record_step, color=\"blue\")\n","plt.grid()\n","plt.xlabel(\"episode\")\n","plt.ylabel(\"step\")\n","\n","plt.savefig(\"./q-learning_episode_per_reward.png\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# 推論"]},{"cell_type":"markdown","metadata":{},"source":["#### 構成\n","1. ライブラリのインポート\n","2. 環境の用意\n","3. 離散化処理\n","4. QTableの読み込み\n","5. メインループ\n","   1. 環境の初期化\n","   2. Qtableから行動選択"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gymnasium as gym\n","import numpy as np\n","import mujoco\n","import time\n","import pickle\n","import matplotlib.pyplot as plt\n","from tqdm import trange\n","import random\n","import keyboard\n","\n","\n","#試験回数\n","test_num = 50\n","time_memo = []\n","\n","\n","\n","'''\n","環境の用意\n","'''\n","env = gym.make('InvertedPendulum-v4')\n","possible_actions = [0,1]\n","\n","\n","# 離散化\n","num_dizitized = 100  # 状態の分割数\n","def bins(clip_min, clip_max, num):\n","    return np.linspace(clip_min, clip_max, num + 1)[1:-1]\n","\n","# 各値を離散値に変換\n","def digitize_state(observation):\n","    cart_pos, cart_v, pole_angle, pole_v = observation\n","    digitized = [\n","        np.digitize(cart_pos, bins=bins(-2.4, 2.4, num_dizitized)),\n","        np.digitize(cart_v, bins=bins(-3.0, 3.0, num_dizitized)),\n","        np.digitize(pole_angle, bins=bins(-0.5, 0.5, num_dizitized)),\n","        np.digitize(pole_v, bins=bins(-2.0, 2.0, num_dizitized))\n","    ]\n","    return sum([x * (num_dizitized**i) for i, x in enumerate(digitized)])\n","\n","# 保存されたQテーブルを読み込む\n","with open('q_table.pkl', 'rb') as f:\n","    q_table = pickle.load(f)\n","\n","def select_action(state):\n","    # 現在の状態に対する最適な行動をQテーブルから選択する\n","    if state in q_table:\n","        action = max(q_table[state], key=q_table[state].get)\n","    else:\n","        # 状態が未知の場合はランダムな行動を選択するなどの戦略を考えることもあります\n","        action = random.choice(possible_actions)\n","\n","    return action\n","\n","print(\"qキー長押しで強制終了\")\n","\n","for Test in trange(test_num):\n","    # 環境の初期化\n","    state,info = env.reset()\n","    observation = state[0],state[1],state[2],state[3],\n","    state = digitize_state(observation)\n","    action = select_action(state)\n","    start_time = time.time()\n","    if keyboard.is_pressed('q'):\n","        break\n","\n","    while True:  # 1試行のループ\n","\n","        action = select_action(state)\n","        observation, reward, done, _, info = env.step((action,))\n","        next_state = digitize_state(observation)\n","        state = next_state\n","\n","        ts = time.time()\n","\n","        if done:\n","            time_memo.append(ts-start_time)\n","            # print('{:.2f}'.format(ts-start_time),\"sec\")\n","            break\n","\n","env.close()\n","\n","# Output result\n","print(\"カートポールが立っていた平均時間を出力：\")\n","print(\"Time_memn:\",'{:.2f}'.format(sum(time_memo)/len(time_memo)),\"sec\")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMNmAtJlq2uaXmau3WQyn85","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
