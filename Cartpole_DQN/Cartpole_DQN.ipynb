{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 動作確認"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gymnasium as gym\n","import numpy as np\n","from tqdm import trange\n","\n","env = gym.make('CartPole-v1', render_mode=\"human\")\n","\n","max_number_of_steps = 1000\n","num_consecutive_iterations = 100\n","num_episodes = 1\n","last_time_steps = np.zeros(num_consecutive_iterations)\n","\n","for episode in trange(num_episodes):\n","    # 環境の初期化\n","    observation = env.reset()\n","    episode_reward = 0\n","\n","    for t in range(max_number_of_steps):\n","        # action = [np.random.choice([-3, 3])]\n","        action = 1\n","\n","        # 行動の実行とフィードバックの取得\n","        observation, reward, done, _, info = env.step(action)\n","        episode_reward += reward\n","\n","        if done:\n","            # print('%d Episode finished after %d time steps / mean %f' % (episode, t + 1,\n","            #     last_time_steps.mean()))\n","            last_time_steps = np.hstack((last_time_steps[1:], [episode_reward]))\n","            break\n","\n","env.close()"]},{"cell_type":"markdown","metadata":{},"source":["# LINE通知"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#LINE BOT  ON\n","\n","import requests\n","\n","class LINENotifyBot(object):\n","    API_URL = 'https://notify-api.line.me/api/notify'\n","    def __init__(self, access_token):\n","        self.__headers = {'Authorization': 'Bearer ' + access_token}\n","\n","    def send(\n","        self,\n","        message,\n","        image=None,\n","        sticker_package_id=None,\n","        sticker_id=None,\n","    ):\n","        payload = {\n","            'message': message,\n","            'stickerPackageId': sticker_package_id,\n","            'stickerId': sticker_id,\n","        }\n","        files = {}\n","        if image != None:\n","            files = {'imageFile': open(image, 'rb')}\n","        r = requests.post(\n","            LINENotifyBot.API_URL,\n","            headers=self.__headers,\n","            data=payload,\n","            files=files,\n","        )\n","print(\"LINE BOTを起動します.\")"]},{"cell_type":"markdown","metadata":{},"source":["# DQN"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gymnasium as gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{},"source":["# 動画保存"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from JSAnimation.IPython_display import display_animation\n","from matplotlib import animation\n","from IPython.display import display\n"," \n"," \n","def display_frames_as_gif(frames):\n","    \"\"\"\n","    Displays a list of frames as a gif, with controls\n","    \"\"\"\n","    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0),\n","               dpi=72)\n","    patch = plt.imshow(frames[0])\n","    plt.axis('off')\n"," \n","    def animate(i):\n","        patch.set_data(frames[i])\n"," \n","    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames),\n","                                   interval=50)\n"," \n","    anim.save('movie_cartpole_DQN.mp4')  # 動画のファイル名と保存です\n","    display(display_animation(anim, default_mode='loop'))"]},{"cell_type":"markdown","metadata":{},"source":["# 準備"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# namedtupleを生成\n","from collections import namedtuple\n"," \n","Transition = namedtuple(\n","    'Transition', ('state', 'action', 'next_state', 'reward'))"]},{"cell_type":"markdown","metadata":{},"source":["# config & ハイパーパラメータ"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch import nn\n","# 定数の設定\n","ENV = 'CartPole-v1'  # 使用する課題名\n","MAX_STEPS = 200  # 1試行のstep数\n","NUM_EPISODES = 1000  # 最大試行回数\n","\n","GAMMA = 0.99  # 時間割引率\n","LEARNING_RATE = 1e-4\n","BATCH_SIZE = 32\n","CAPACITY = 10000\n","NUM_NEURON = 2048\n","CRITERION = nn.MSELoss()\n","\n","memory_loss = []\n","update_count = []\n","\n","memory_step_mean = []\n","episode_count = []"]},{"cell_type":"markdown","metadata":{},"source":["# ReplayMemory class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 経験を保存するメモリクラスを定義します\n","class ReplayMemory:\n"," \n","    def __init__(self, CAPACITY):\n","        self.capacity = CAPACITY  # メモリの最大長さ\n","        self.memory = []  # 経験を保存する変数\n","        self.index = 0  # 保存するindexを示す変数\n"," \n","    def push(self, state, action, state_next, reward):\n","        \"\"\"state, action, state_next, rewardをメモリに保存します\"\"\"\n"," \n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)  # メモリに保存されている経験が，最大まで満たされていない場合は，新たなに枠を確保する．\n"," \n","        # namedtupleのTransitionを使用し、値とフィールド名をペアにして保存します\n","        self.memory[self.index] = Transition(state, action, state_next, reward)\n"," \n","        self.index = (self.index + 1) % self.capacity  # 保存するindexを1つずらす\n"," \n","    def sample(self, batch_size):\n","        \"\"\"batch_size分だけ、ランダムに保存内容を取り出します\"\"\"\n","        return random.sample(self.memory, batch_size)\n"," \n","    def __len__(self):\n","        return len(self.memory)"]},{"cell_type":"markdown","metadata":{},"source":["# Brain class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# エージェントが持つ脳となるクラスです、DQNを実行\n","# Q関数をディープラーニングのネットワークをクラスとして定義\n","import random\n","import torch\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n"," \n","class Brain:\n","    def __init__(self, num_states, num_actions):\n","        self.update_time = 0\n","        self.num_states = num_states  # CartPoleは状態数4を取得\n","        self.num_actions = num_actions  # CartPoleの行動（右に左に押す）の2を取得\n"," \n","        # 経験を記憶するメモリオブジェクトを生成\n","        self.memory = ReplayMemory(CAPACITY)\n"," \n","        # ニューラルネットワークを構築\n","        self.model = nn.Sequential()\n","        self.model.add_module('fc1', nn.Linear(self.num_states, NUM_NEURON)) #512\n","        self.model.add_module('relu1', nn.ReLU())\n","        # self.model.add_module('fc2', nn.Linear(NUM_NEURON, NUM_NEURON))\n","        # self.model.add_module('relu2', nn.ReLU())\n","        self.model.add_module('fc3', nn.Linear(NUM_NEURON, self.num_actions))\n"," \n","        print(self.model)  # ネットワークの形を出力\n"," \n","        # 最適化手法の設定\n","        self.optimizer = optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n"," \n","    def replay(self):\n","        \"\"\"Experience Replayでネットワークの重みを学習 \"\"\"\n"," \n","        # メモリサイズがミニバッチより小さい間は何もしない\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n"," \n","        # メモリからミニバッチ分のデータを取り出す\n","        transitions = self.memory.sample(BATCH_SIZE)\n"," \n","        # ミニバッチの作成-----------------\n"," \n","        # transitionsは1stepごとの(state, action, state_next, reward)が、BATCH_SIZE分格納されている\n","        # つまり、(state, action, state_next, reward)×BATCH_SIZE\n","        # これをミニバッチにしたい。つまり\n","        # (state×BATCH_SIZE, action×BATCH_SIZE, state_next×BATCH_SIZE, reward×BATCH_SIZE)にする\n","        batch = Transition(*zip(*transitions))\n"," \n","        # cartpoleがdoneになっておらず、next_stateがあるかをチェックするマスクを作成\n","        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None,batch.next_state)))\n"," \n","        # バッチから状態、行動、報酬を格納（non_finalはdoneになっていないstate）\n","        # catはConcatenates（結合）のことです。\n","        # 例えばstateの場合、[torch.FloatTensor of size 1x4]がBATCH_SIZE分並んでいるのですが、\n","        # それを size BATCH_SIZEx4 に変換します\n","\n","        # print(batch.next_state)\n","\n","\n","        state_batch = Variable(torch.cat(batch.state))\n","        action_batch = Variable(torch.cat(batch.action))\n","        reward_batch = Variable(torch.cat(batch.reward))\n","        non_final_next_states = Variable(torch.cat([s for s in batch.next_state if s is not None]))\n"," \n","        # ミニバッチの作成終了------------------\n"," \n","        # ネットワークを推論モードに切り替える\n","        self.model.eval()\n"," \n","        # Q(s_t, a_t)を求める\n","        # self.model(state_batch)は、[torch.FloatTensor of size BATCH_SIZEx2]になっており、\n","        # 実行したアクションに対応する[torch.FloatTensor of size BATCH_SIZEx1]にするために\n","        # gatherを使用します。\n","        state_action_values = self.model(state_batch).gather(1, action_batch)\n"," \n","        # max{Q(s_t+1, a)}値を求める。\n","        # 次の状態がない場合は0にしておく\n","        next_state_values = Variable(torch.zeros(BATCH_SIZE).type(torch.FloatTensor))\n"," \n","        # 次の状態がある場合の値を求める\n","        # 出力であるdataにアクセスし、max(1)で列方向の最大値の[値、index]を求めます\n","        # そしてその値（index=0）を出力します\n","        next_state_values[non_final_mask] = self.model(\n","            non_final_next_states).data.max(1)[0]\n"," \n","        # 教師となるQ(s_t, a_t)値を求める\n","        expected_state_action_values = reward_batch + GAMMA * next_state_values\n"," \n","        # ネットワークを訓練モードに切り替える\n","        self.model.train()\n"," \n","\n","        # criterion = nn.MSELoss()\n","\n","        # 損失関数を計算する。smooth_l1_lossはHuberlossです\n","        # loss = F.smooth_l1_loss(state_action_values,\n","        #                         expected_state_action_values)\n","        loss = CRITERION(state_action_values, expected_state_action_values)\n","        \n","        self.update_time +=  1\n","        memory_loss.append(loss.detach().numpy())\n","        update_count.append(self.update_time)\n"," \n","        # ネットワークを更新します\n","        self.optimizer.zero_grad()  # 勾配をリセット\n","        loss.backward()  # バックプロパゲーションを計算\n","        self.optimizer.step()  # 結合パラメータを更新\n"," \n","    def decide_action(self, state, episode):\n","        # ε-greedy法で徐々に最適行動のみを採用する\n","        # epsilon = 0.5 * (1 / (episode + 1))\n","        epsilon = 0.02\n"," \n","        if epsilon <= np.random.uniform(0, 1):\n","            self.model.eval()  # ネットワークを推論モードに切り替える\n","            action = self.model(Variable(state)).data.max(1)[1].view(1, 1)\n","            # ネットワークの出力の最大値のindexを取り出します = max(1)[1]\n","            # .view(1,1)はtorch.LongTensor of size 1　を size 1x1 に変換します\n"," \n","        else:\n","            # 0,1の行動をランダムに返す\n","            action = torch.LongTensor(\n","                [[random.randrange(self.num_actions)]])  # 0,1の行動をランダムに返す\n","            # actionは[torch.LongTensor of size 1x1]の形になります\n"," \n","        return action"]},{"cell_type":"markdown","metadata":{},"source":["# Agent class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Agent:\n","    def __init__(self, num_states, num_actions):\n","        \"\"\"課題の状態と行動の数を設定します\"\"\"\n","        self.num_states = num_states  # CartPoleは状態数4を取得\n","        self.num_actions = num_actions  # CartPoleの行動（右に左に押す）の2を取得\n","        self.brain = Brain(num_states, num_actions)  # エージェントが行動を決定するための頭脳を生成\n","\n","    def update_q_function(self):\n","        \"\"\"Q関数を更新します\"\"\"\n","        self.brain.replay()\n","\n","    def get_action(self, state, step):\n","        \"\"\"行動の決定します\"\"\"\n","        action = self.brain.decide_action(state, step)\n","        return action\n","\n","    def memorize(self, state, action, state_next, reward):\n","        \"\"\"memoryオブジェクトに、state, action, state_next, rewardの内容を保存します\"\"\"\n","        self.brain.memory.push(state, action, state_next, reward)"]},{"cell_type":"markdown","metadata":{},"source":["# Environment class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Environment:\n"," \n","    def __init__(self):\n","        self.env = gym.make(ENV)  # 実行する課題を設定, render_mode=\"human\"\n","        self.num_states = self.env.observation_space.shape[0]  # 課題の状態と行動の数を設定\n","        self.num_actions = self.env.action_space.n  # CartPoleの行動（右に左に押す）の2を取得\n","        # 環境内で行動するAgentを生成\n","        self.agent = Agent(self.num_states, self.num_actions)\n","        self.total_step = np.zeros(10)  # 10試行分の立ち続けたstep数を格納し、平均ステップ数を出力させます\n"," \n","    def run(self):\n","        \"\"\"メインの実行\"\"\"\n"," \n","        complete_episodes = 0  # 195step以上連続で立ち続けた試行数\n","        episode_final = False  # 最後の試行フラグ\n","        frames = []  # 最後の試行を動画にするために画像を格納する変数\n"," \n","        for episode in range(NUM_EPISODES):  # 試行数分繰り返す\n","            observation = self.env.reset()  # 環境の初期化\n","            state,_ = observation  # 観測をそのまま状態sとして使用\n","            state = torch.from_numpy(state).type(\n","                torch.FloatTensor)  # numpy変数をPyTorchのテンソルに変換\n"," \n","            # 今、FloatTensorof size 4になっているので、size 1x4に変換\n","            state = torch.unsqueeze(state, 0)\n"," \n","            for step in range(MAX_STEPS):  # 1エピソードのループ\n","                if episode_final is True:\n","                    \"\"\"framesに各時刻の画像を追加していく\"\"\"\n","                    frames.append(self.env.render(mode='rgb_array'))\n"," \n","                action = self.agent.get_action(state, episode)  # 行動を求める\n"," \n","                # 行動a_tの実行により、s_{t+1}とdoneフラグを求める\n","                # actionは、torch.LongTensor of size 1x1になっているので、[0,0]を指定して、中身を取り出す\n","                observation_next, _, done, _, _ = self.env.step(action[0, 0].item())\n"," \n","                # episodeの終了評価と、state_nextを設定\n","                if done:  # ステップ数が200経過するか、一定角度以上傾くとdoneはtrueになる\n","                    state_next = None  # 次の状態はないので、Noneを格納\n","                    self.total_step = np.hstack((self.total_step[1:], step + 1))  # step数を保存\n","\n","                    if step < 190:\n","                        reward = torch.FloatTensor([-200.0])  # 途中でこけたら罰則として報酬-1を与える\n","                        self.complete_episodes = 0  # 連続成功記録をリセット\n","                    else:\n","                        reward = torch.FloatTensor([1.0])  # 立ったまま終了時は報酬1を与える\n","                        self.complete_episodes = self.complete_episodes + 1  # 連続記録を更新\n","\n","                else:\n","                    reward = torch.FloatTensor([1.0])  # 普段は報酬0\n","                    state_next = observation_next  # 観測をそのまま状態とする\n","                    state_next = torch.from_numpy(state_next).type(torch.FloatTensor)  # numpyとPyTorchのテンソルに\n"," \n","                    # テンソルがsize 4になっているので、size 1x4に変換\n","                    state_next = torch.unsqueeze(state_next, 0)\n"," \n","                # メモリに経験を追加\n","                self.agent.memorize(state, action, state_next, reward)\n"," \n","                # Experience ReplayでQ関数を更新する\n","                self.agent.update_q_function()\n"," \n","                # 観測の更新\n","                state = state_next\n"," \n","                # 終了時の処理\n","                if done:\n","                    print('%d Episode: Finished after %d steps：10Average = %.1lf' % (episode, step + 1, self.total_step.mean()))\n","                    episode_count.append(episode + 1)\n","                    memory_step_mean.append(self.total_step.mean())\n","\n","                    break\n"," \n","            if episode_final is True:\n","                # 動画を保存と描画\n","                display_frames_as_gif(frames)\n","                break\n"," \n","            # 10連続で200step立ち続けたら成功\n","            if self.complete_episodes >= 10:\n","                print('10回連続成功')\n","                episode_final = True  # 次の試行を描画を行う最終試行とする"]},{"cell_type":"markdown","metadata":{},"source":["# main"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# main\n","cartpole_env = Environment()\n","cartpole_env.run()"]},{"cell_type":"markdown","metadata":{},"source":["# プロット"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["env.close()\n","plt.figure(tight_layout=True)\n","plt.subplot(1, 2, 1)\n","plt.plot(update_count,memory_loss, color=\"red\")\n","plt.grid()\n","plt.xlabel(\"updata\")\n","plt.ylabel(\"loss\")\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(episode_count, memory_step_mean, color=\"blue\")\n","plt.grid()\n","plt.xlabel(\"episode\")\n","plt.ylabel(\"step\")\n","\n","plt.savefig(\"img/test.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import json\n","import datetime\n","\n","\n","dt_now = datetime.datetime.now()\n","\n","\n","with open(\"./settings.json\", \"r\", encoding=\"utf-8\") as f:\n","        j = json.load(f)\n","\n","token = j[\"token\"][\"my_token\"]\n","\n","bot = LINENotifyBot(access_token = token)\n","bot.send(\n","message=\"トレーニング完了 \\n \"\n","        +\"Data:\"+str(dt_now)+\"\\n\"\n","        +\"ENV:\"+str(ENV)+\"\\n\"\n","        +\"CRITERION:\"+str(CRITERION)+\"\\n\"\n","        +\"NUM_NEURON:\"+str(NUM_NEURON)+\"\\n\"\n","        +\"GAMMA:\"+str(GAMMA)+\"\\n\"\n","        +\"LEARNING_RATE:\"+str(LEARNING_RATE)+\"\\n\"\n","        +\"MAX_STEPS:\"+str(MAX_STEPS)+\"\\n\"\n","        +\"MAX_STEPS:\"+str(NUM_EPISODES)+\"\\n\"\n","        +\"BATCH_SIZE:\"+str(BATCH_SIZE)+\"\\n\"\n","        +\"CAPACITY:\"+str(CAPACITY)+\"\\n\"\n","        ,\n","image='img/test.png'\n","\n",")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMNmAtJlq2uaXmau3WQyn85","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
