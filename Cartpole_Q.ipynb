{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12521,"status":"ok","timestamp":1696668226912,"user":{"displayName":"r rrr","userId":"14222254183072818856"},"user_tz":-540},"id":"iPdUtXDM-UZf","outputId":"9d06aead-f3e6-4ef4-9fa7-9ce6413fb000"},"outputs":[],"source":["# !pip install mujoco\n","# !pip install pip install gymnasium[mujoco]"]},{"cell_type":"markdown","metadata":{},"source":["### 概要\n","Mujoco環境下でのカートポールのQ学習による学習とその推論が行えるサンプル\n","\n","### 注意事項\n","pkl形式で保存されるQTableは，パラメータによっては1GB以上の大きさになるので注意！"]},{"cell_type":"markdown","metadata":{},"source":["# Q学習"]},{"cell_type":"markdown","metadata":{},"source":["#### 構成\n","1. ライブラリのインポート\n","1. ハイパーパラメータの定義\n","1. 環境の用意\n","1. 離散化処理\n","1. Q値の初期化\n","2. メインループ\n","   1. ε-greedy\n","   2. 状態と報酬の取得\n","   3. Q値の更新\n","   4. 状態の更新"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gymnasium as gym\n","import numpy as np\n","import mujoco\n","import time\n","import pickle\n","import matplotlib.pyplot as plt\n","from tqdm import trange\n","from collections import defaultdict\n","\n","# グラフ描画用の配列\n","record_episode = []\n","record_reward = []\n","record_step = []\n","model = defaultdict(lambda: dict())\n","\n","'''\n","ハイパーパラメータ\n","'''\n","ALPHA = 0.05\n","GAMMA = 0.95 # 割引率\n","EPSILON = 0.1 # ε-greedy法のε\n","max_number_of_steps = 1000  # 1試行のstep数\n","num_consecutive_iterations = 5  # 学習完了評価に使用する平均試行回数\n","num_episodes = 80000  # 総試行回数\n","total_reward_vec = np.zeros(num_consecutive_iterations)  # 各試行の報酬を格納\n","num_planning_steps = 50 #プランニングステップ数\n","\n","start_time = time.time()\n","\n","rng = np.random.RandomState()\n","\n","'''\n","環境の用意\n","'''\n","env = gym.make('InvertedPendulum-v4')#, render_mode=\"human\" CartPole-v1\n","print(env.action_space.shape)\n","observation, info = env.reset()\n","\n","'''\n","離散化\n","'''\n","num_dizitized = 5  # 状態の分割数\n","def bins(clip_min, clip_max, num):\n","    return np.linspace(clip_min, clip_max, num + 1)[1:-1]\n","\n","# 各値を離散値に変換\n","def digitize_state(observation):\n","    cart_pos, cart_v, pole_angle, pole_v = observation\n","    digitized = [\n","        np.digitize(cart_pos, bins=bins(-2.4, 2.4, num_dizitized)),\n","        np.digitize(cart_v, bins=bins(-3.0, 3.0, num_dizitized)),\n","        np.digitize(pole_angle, bins=bins(-0.5, 0.5, num_dizitized)),\n","        np.digitize(pole_v, bins=bins(-2.0, 2.0, num_dizitized))\n","    ]\n","    return sum([x * (num_dizitized**i) for i, x in enumerate(digitized)])\n","\n","\n","q_table = np.random.uniform(low=-1, high=1, size=(num_dizitized**4, 2))\n","\n","\n","'''\n","学習の実行\n","'''\n","# ---epsilon-グリーディ\n","def get_action_q(next_state, episode, epsilon):\n","    if epsilon <= np.random.uniform(0, 1):\n","        next_action = np.argmax(q_table[next_state])\n","    else:\n","        next_action = np.random.choice([0, 1])\n","    return next_action\n","\n","# ---Q学習関連\n","def update_Qtable(q_table, state, action, reward, next_state):\n","    next_Max_Q=max(q_table[next_state][0],q_table[next_state][1] )\n","    q_table[state, action] = (1 - ALPHA) * q_table[state, action] + ALPHA * (reward + GAMMA * next_Max_Q)\n","    return q_table\n","\n","# ---Dyna\n","def add_experience_to_model(state, action, next_state, reward):\n","    model[state][action] = tuple([next_state, reward])\n","\n","def sample_from_model():\n","    state = rng.choice(list(model.keys()))\n","    action = rng.choice(list(model[state].keys()))\n","    next_state, reward = model[state][action]\n","    return state, action, next_state, reward\n","\n","\n","ts = time.time()\n","for episode in trange(num_episodes, leave=False):  # 試行数分繰り返す\n","    # 環境の初期化\n","    state,info = env.reset()\n","    observation = state[0],state[1],state[2],state[3],\n","    state = digitize_state(observation)\n","    action = np.argmax(q_table[state])\n","    episode_reward = 0\n","\n","\n","    for t in trange(max_number_of_steps, leave=False):  # 1試行のループ\n","        # print(action,state)\n","        observation, reward, done, _, info = env.step((action,))\n","        # 状態を保存\n","        # state_data.append((observation, action, reward)) # 現在\n","        # render_data.append(env.render()) # 次\n","        # episode_data.append(episode)\n","        step_num = 0\n","\n","        # 報酬設計\n","        if done:\n","            if t < 195: # step = 195　で終了にしている．\n","                reward = 0  # 倒れたら罰則\n","            else:\n","                reward = 1  # 立ったまま終了時は罰則はなし\n","        else:\n","            reward = 1  # 各ステップで立ってたら報酬追加\n","\n","        episode_reward += reward  # 報酬を追加\n","\n","        # Q-tableを更新する\n","        next_state = digitize_state(observation)  # 観測状態を離散値に変換\n","        q_table = update_Qtable(q_table, state, action, reward, next_state)\n","        #Dyna-Q\n","        add_experience_to_model(state, action, next_state, reward)\n","        for i in range(num_planning_steps):\n","            state_in_model, action_in_model, next_state_in_model, reward_in_model = sample_from_model()\n","            update_Qtable(q_table, state_in_model, action_in_model, next_state_in_model, reward_in_model)\n","            # state, action, next_state, reward\n","\n","        #  行動を選択\n","        action = get_action_q(next_state, episode, EPSILON)\n","\n","        print(action)\n","\n","        state = next_state\n","        step_num += 1\n","        # 終了時の処理\n","        if done:\n","            # if (episode+1)%10==0:\n","            #   print('Episode {0}: {1} steps, reward {2}, mean reward {3:.3f}'.format(episode+1, t+1, episode_reward, total_reward_vec.mean()))\n","            total_reward_vec = np.hstack((total_reward_vec[1:], episode_reward))  # 報酬を記録\n","            record_reward.append(total_reward_vec.mean()) # グラフ描画用に記録\n","            record_episode.append(episode+1)\n","            step_num = t\n","            record_step.append(step_num+1)\n","            break\n","env.close()\n","\n","# Qテーブルを保存する\n","with open('q_table.pkl', 'wb') as f:\n","    pickle.dump(q_table, f)\n","\n","# プロット\n","fig = plt.figure()\n","plt.subplot(1, 2, 1)\n","plt.plot(record_episode, record_reward, color=\"red\")\n","plt.grid()\n","plt.xlabel(\"episode\")\n","plt.ylabel(\"mean reward\")\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(record_episode, record_step, color=\"blue\")\n","plt.grid()\n","plt.xlabel(\"episode\")\n","plt.ylabel(\"step\")\n","\n","plt.savefig(\"./q-learning_episode_per_reward.png\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# 推論"]},{"cell_type":"markdown","metadata":{},"source":["#### 構成\n","1. ライブラリのインポート\n","2. 環境の用意\n","3. 離散化処理\n","4. QTableの読み込み\n","5. メインループ\n","   1. 環境の初期化\n","   2. Qtableから行動選択"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gymnasium as gym\n","import numpy as np\n","import mujoco\n","import time\n","import pickle\n","import matplotlib.pyplot as plt\n","from tqdm import trange\n","import random\n","import keyboard\n","\n","\n","#試験回数\n","test_num = 50\n","time_memo = []\n","\n","\n","\n","'''\n","環境の用意\n","'''\n","env = gym.make('InvertedPendulum-v4')\n","possible_actions = [0,1]\n","\n","\n","# 離散化\n","num_dizitized = 100  # 状態の分割数\n","def bins(clip_min, clip_max, num):\n","    return np.linspace(clip_min, clip_max, num + 1)[1:-1]\n","\n","# 各値を離散値に変換\n","def digitize_state(observation):\n","    cart_pos, cart_v, pole_angle, pole_v = observation\n","    digitized = [\n","        np.digitize(cart_pos, bins=bins(-2.4, 2.4, num_dizitized)),\n","        np.digitize(cart_v, bins=bins(-3.0, 3.0, num_dizitized)),\n","        np.digitize(pole_angle, bins=bins(-0.5, 0.5, num_dizitized)),\n","        np.digitize(pole_v, bins=bins(-2.0, 2.0, num_dizitized))\n","    ]\n","    return sum([x * (num_dizitized**i) for i, x in enumerate(digitized)])\n","\n","# 保存されたQテーブルを読み込む\n","with open('q_table.pkl', 'rb') as f:\n","    q_table = pickle.load(f)\n","\n","def select_action(state):\n","    # 現在の状態に対する最適な行動をQテーブルから選択する\n","    if state in q_table:\n","        action = max(q_table[state], key=q_table[state].get)\n","    else:\n","        # 状態が未知の場合はランダムな行動を選択するなどの戦略を考えることもあります\n","        action = random.choice(possible_actions)\n","\n","    return action\n","\n","print(\"qキー長押しで強制終了\")\n","\n","for Test in trange(test_num):\n","    # 環境の初期化\n","    state,info = env.reset()\n","    observation = state[0],state[1],state[2],state[3],\n","    state = digitize_state(observation)\n","    action = select_action(state)\n","    start_time = time.time()\n","    if keyboard.is_pressed('q'):\n","        break\n","\n","    while True:  # 1試行のループ\n","\n","        action = select_action(state)\n","        observation, reward, done, _, info = env.step((action,))\n","        next_state = digitize_state(observation)\n","        state = next_state\n","\n","        ts = time.time()\n","\n","        if done:\n","            time_memo.append(ts-start_time)\n","            # print('{:.2f}'.format(ts-start_time),\"sec\")\n","            break\n","\n","env.close()\n","\n","# Output result\n","print(\"カートポールが立っていた平均時間を出力：\")\n","print(\"Time_memn:\",'{:.2f}'.format(sum(time_memo)/len(time_memo)),\"sec\")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMNmAtJlq2uaXmau3WQyn85","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
